%!TEX root=../master.tex
%========================================================================================================
% MOCK
%========================================================================================================
\newline
\newline
\section{Mock Exam Notes}
\subsection{Normal equation}
Unique if \textit{strictly} convex.\newline
$\frac{1}{\sigma_k^2} X(X^Tw_k-y_k)+w_k = 0 \Leftrightarrow$ \newline
$ w_k^* = (\frac{1}{\sigma_k^2} XX^T+I_D)^{-1} \frac{1}{\sigma_k^2}Xy_k$

\subsection{MAP solution}
$\mathcal{L}(w) = \sum_k \sum_n \frac{1}{2\sigma_k^2} (y_{nk} - x_n^T w_k)^2 + \frac{1}{2} \sum_k ||w_k||^2_2 \rightarrow$
Likelihood $p(y|X,w) = \prod_n \prod_k \mathcal{N}(y_{nk}|w_k^Tx_n, \sigma_k^2)$ and prior $p(w) = \prod_k \mathcal{N}(w_k|0,I_D)$

\subsection{Deriving marginal distribution}
$p(y_n|x_n,r_n=k,\beta) = \mathcal{N} (y_n|\beta_k^T\tilde{x}_n,1)$
Assume $r_n$ follows a multinomial $p(r_n=k|\pi)$. Derive the marginal $p(y_n|x_n,\beta,\pi)$.
$p(y_n|x_n,r_n=k,\beta) = \sum_k^K p(y_n,r_n=k|x_n,\beta,\pi) = \sum_k^K p(y_n|r_n=k,x_n,\beta,\pi) \cdot \pi_k = \sum_k^K \mathcal{N}(y_n|\beta_k^T\tilde{x}_n, \sigma^2)\cdot \pi_k$

\subsection{MF}
$\hat{r}_{um} = \langle\,\mathbf{v}_u,\mathbf{w}_m\rangle + b_u + b_m$

$\mathcal{L} = \frac{1}{2} \sum_{u ~ m}(\hat{r}_{um} -r_{um}) + \frac{\lambda}{2} \big[
\sum_u (b_u^2+||\mathbf{v}_u||^2) + \sum_m (b_m^2+||\mathbf{w}_m||^2) \big]$. The optimal value for $b_u$ for a particular user $u'$ : $\sum_{u' ~ m} (\hat{r}_{u'm} - r_{u'm}) + \lambda b_{u'} = 0$.

Problem jointly convex? Compute $H(\hat{r}({v,w})) = \begin{bmatrix} 2w^2 & 4vw-2r \\ 4vw-2r & 2v^2 \end{bmatrix}$ which is not PSD in general.

%========================================================================================================
% QCM
%========================================================================================================

\section{Multiple Choice Notes}
\subsection{True statements}
\begin{itemize}
\item Regularization term $\rightarrow$ sometimes min to cvx problem.
\item k-NN even data not lin sep.
\item $max\{ 0, x\} = \underset{{\alpha \in [0,1]}}{max} \,\, \alpha x$ \newline $min\{ 0, x\} = \underset{{\alpha \in [0,1]}}{min}\,\,  \alpha x$
\item $g(x) = \underset{y}{min}\,\, f(x,y) \Rightarrow g(x) \le f(x,y)$
\item $\underset{x}{max}\,\, g(x) \le \underset{x}{max} \,\,f(x,y)$ \newline
\item $\underset{x}{max} \,\, \underset{y}{min} \,\, f(x,y) \newline \le \underset{y}{min} \,\, \underset{x}{max} \,\, f(x,y)$\newline
\item $\nabla_{W} (\mathbf{x}^T\mathbf{W}\mathbf{x}) = \nabla_{W} (\sum_{i,j}W_{i,j}x_ix_j) = \mathbf{x}\mathbf{x}^T$
\item $\nabla_{x} (\mathbf{x}^T\mathbf{W}\mathbf{x}) = (\mathbf{W}+\mathbf{W}^T)\mathbf{x}$
\item K-means: opt cluster (centers) init $\rightarrow$ one step opt representation points (clusters).
\item Logistic loss is typically preferred over $L_2$ loss in classification.
\item For optimising a MF of a $D\times N$ matrix, for large $D$, $N$ : \textit{per iteration, cost(ALS) $>$ cost(SGD)} and \textit{per iteration, SGD cost $\neq f(D, N)$}.
\item The complexity of backprop for a nn with $L$ layers and $K$ nodes/layer is $O(K^2L)$
\item One-dimensional CNN with filter/kernel $M$ non-zero terms. Without bias terms, $M$ parameters per layer.
\end{itemize}

\subsection{Convex functions}
\begin{itemize}
\item $f(x) = x^{\alpha}, x \in \mathbb{R^+}, \forall \alpha \ge 1 or \le 0$
\item $f(x) = -x^3, x \in [-1,0]$
\item $f(x) = e^{ax}, \forall x,a \in \mathbb{R}$
\item $f(x) = ln(1/x), x \in \mathbb{R^+}$
\item $f(x) = g(h(x)), x \in \mathbb{R}, g,h$ convex and increasing over $\mathbb{R}$
\item $f(x) = ax+b, x \in \mathbb{R}, \forall a,b \in \mathbb{R}$
\item $f(x) = |x|^p, x \in \mathbb{R}, p\ge 1$
\item $f(x) = xlog(x), x \in \mathbb{R}^+$
\item $ln[\sum_k^Ke^{t_k}]$
\end{itemize}

%\subsection{Non-convex functions}
%\begin{itemize}
%\item $f(x) = x^3, x \in [-1,1]$
%\item $f(x) = e^{-x^2}, x \in \mathbb{R}$
%\item $\sum \mathcal{N}$, $sin(x)\,\, , \forall x \in \mathbb{R}$ \newline
%\end{itemize}
