%!TEX root=../master.tex
\section{Quick maff}
Chain rule $h = f(g(w)) \rightarrow \partial h(w) = \partial f(g(w)) \nabla g(w)$

Gaussian $\mathcal{N}(y|\mu, \sigma^2) \frac{1}{\sqrt{2\pi \sigma^2}} exp(-\frac{(y-\mu)^2}{\sigma^2})$

Multivariate Gaussian $\mathcal{N}(y|\mu, \sigma^2) \frac{1}{\sqrt{(2\pi)^D det(\Sigma)}} exp(-\frac{1}{2} (y-\mu)^T \Sigma^{-1} (y-\mu))$

Bayes rule $p(x|y) = \frac{p(y|x) p(x)}{p(y)}$

Logit $\sigma(x) = \frac{\partial ln[1+e^x]}{\partial x}$

Naming
Joint distribution $p(x,y) = p(x|y)p(y)=p(y|x)p(x)$ where
\begin{itemize}
\item $p(x|y) \rightarrow$ likelihood 
\item $p(y) \rightarrow$ prior
\item $p(y|x) \rightarrow$ posterior
\item $p(x) \rightarrow$ marginal likelihood
\end{itemize}

Marginal Likelihood \newline $p({\mathbb  {X}}|\alpha )=\int _{\theta }p({\mathbb  {X}}|\theta )\,p(\theta |\alpha )\ \operatorname {d}\!\theta $

Posterior probability $\propto$ Likelihood $\times$ Prior

Maximising over a Gaussian is equivalent to minimising MSE: $\beta_{MAP}^* = arg max_{\beta} p(y|X,\beta)p(\beta) \Leftrightarrow \beta^* = arg min_{\beta} \mathcal{L}(\beta)$

Identifiable model $\theta_1 = \theta_2 \rightarrow P_{\theta_1} = P_{\theta_2}$

\subsection{Algebra}
$(PQ + I_N)^{-1} P = P (QP + I_M)^{-1}$

$\sum_n (y_{n} -\mathbf{\beta}^T\mathbf{x_n})^2 = (\mathbf{y} - \mathbf{X}\mathbf{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta})$

$\sum_j \beta^2 = \mathbf{\beta}^T \mathbf{\beta}$