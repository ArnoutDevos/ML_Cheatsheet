%!TEX root=../master.tex
%========================================================================================================
% REGRESSION
%========================================================================================================
\section{Regression}
\subsection{Linear Regression}
% Simple
% $y_n \approx f(\mathbf{x_n}) := w_0 + w_1 x_{n1}$\newline
% Multiple\newline
$f(\mathbf{x_n}) := w_0 + \sum_{j=1}^D w_j x_{nj} = \tilde{\mathbf{x}}_n^T \mathbf{w}$\newline
If $D > N$ the task is under-determined (more dimensions than data) $\rightarrow$ regularisation.

\section{Cost functions}
MSE $= \frac{1}{N} \sum_{n=1}^N [y_n - f(\mathbf{x_n})]^2$ \newline
MAE $= \frac{1}{N} \sum_{n=1}^N |y_n - f(\mathbf{x_n})|$\newline
% Error $e_n = y_n - f(\mathbf{x_n})$
\subsection{Convexity}
% A line joining two points never intersects with the function anywhere else.
$f(\lambda \mathbf{u} + (1-\lambda)\mathbf{v}) \le \lambda f(\mathbf{u}) + (1-\lambda) f(\mathbf{v})$ with $\lambda \in [0;1]$.
A strictly convex function has a unique global minimum $w^*$. 
% Sums of convex functions are convex. 
A function must always lie above its linearisation: \newline $\mathcal{L}(u) \ge \mathcal{L}(w) + \nabla \mathcal{L}(w)^T (u-w) \forall u,w$.

A set is convex iff line segment between any two points of $\mathcal{C}$ lies in $\mathcal{C}$ : $\theta u + (1 - \theta) v \in \mathcal{C}$

%========================================================================================================
% OPTIM
%========================================================================================================
\section{Optimisation}
% Find $\mathbf{w^*} \in \mathcal{R}^D$ which $min\,\,\mathcal{L}(\mathbf{w})$.\newline
Gradient $\nabla \mathcal{L} := \begin{bmatrix} \frac{\partial \mathcal{L}(\mathbf{w})}{\partial w_1} & \dots & \frac{\partial \mathcal{L}(\mathbf{w})}{\partial w_D} \end{bmatrix}$

\subsection{Gradient descent}

$\mathbf{w^{(t+1)}} = \mathbf{w^{(t)}} - \gamma \nabla \mathcal{L}(\mathbf{w^{(t)}})$. Very sensitive to ill-conditioning.

GD - Linear Reg

$\mathcal{L}(\mathbf{w}) = \frac{1}{2N} (\mathbf{y} - X\mathbf{w})^T(\mathbf{y} - X\mathbf{w}) \rightarrow$ \newline $ \nabla \mathcal{L}(\mathbf{w}) = - \frac{1}{N} X^T(\mathbf{y} - X\mathbf{w})$. Cost: \newline $O_{err} = 2ND + N$ and $O_{w} = 2ND + D$.

\subsection{SGD}

$\mathcal{L} = \frac{1}{N} \sum{\mathcal{L}}_n(\mathbf{w})$ with update \newline $\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \gamma \nabla \mathcal{L}_n(\mathbf{w}^{(t)})$.

\subsection{Mini-batch SGD}

$\mathbf{g} = \frac{1}{|B|} \sum_{n\in B}{\nabla \mathcal{L}}_n(\mathbf{w}^{(t)})$ with update \newline $\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \gamma \mathbf{g}$.

\subsection{Subgradient at $w$}

$\mathbf{g} \in \mathbb{R}^D$ with $\mathcal{L}(u) \ge \mathcal{L}(w) + \mathbf{g}^T (u-w)$. 

% Example subgradient for MAE: $h(e) = |e| \rightarrow g(e) = {sgn(e) \text{ if } e \ne 0, \lambda \in [-1;1] \text{ otherwise }}$. We get the gradient:\newline $\nabla \mathcal{L}_{MAE} = - \frac{1}{N} \sum_n sgn(y-f(x_n)) \nabla f(x_n)$.

\subsection{Projected SGD}

$\mathbf{w}^{(t+1)} = \mathcal{P_C} [\mathbf{w}^{(t)} - \gamma \nabla \mathcal{L}(\mathbf{w}^{(t)})]$

\subsection{Newton's method}
2nd order (more expensive \newline $O(ND^2 + D^3)$ but faster convergence).

$w^{(t+1)} = w^{(t)} - \gamma^{(t)} (H^{(t)})^{-1} \nabla \mathcal{L}(w^{(t)})$

\subsection{Optimality conditions}
Necessary : $\nabla \mathcal{L} (\mathbf{w}^*) = 0$
Sufficient : Hessian PSD $\mathbf{H}(\mathbf{w}^*) := \frac{\partial^2 \mathcal{L}(\mathbf{w}^*)}{\partial w \partial w^T}$
\newline
%========================================================================================================
% LEAST SQUARES
%========================================================================================================

\section{Least Squares}
\subsection{Normal Equation}
$X^T (\mathbf{y} - X\mathbf{w})= 0 \Rightarrow$\newline$\mathbf{w^*} = (X^TX)^{-1}X^T\mathbf{y} \text{ and } \mathbf{\hat{y}_m} = \mathbf{x_m}^T \mathbf{w^*}$
Graham matrix invertible iff $rank(X) = D$ (use SVD $X = USV^T \in \mathbb{R}^{N\times D}$ if this is not the case to get pseudo-inverse $\mathbf{w^*} = V\tilde{S}U^Ty$ with $\tilde{S}$ pseudo-inverse of $S$).

\section{Likelihood}
Probabilistic model $y_n = \mathbf{x_n}^T\mathbf{w} + \epsilon_n$.
Probability of observing the data given a set of parameters and inputs :
$p(\mathbf{y}|X, \mathbf{w}) = \prod p(y_n|\mathbf{x_n}, \mathbf{w})  = \prod \mathcal{N} (y_n | \mathbf{x_n}^T\mathbf{w}, \sigma^2)$

Best model maximises log-likelihood $\mathcal{L}_{LL} = -\frac{1}{2\sigma^2} \sum(y_n-x_n^Tw)^2+cst$.


%========================================================================================================
% REGULARISATION
%========================================================================================================
\section{Regularisation}
\subsection{Ridge Regression}
$\mathcal{L}(\mathbf{w}) = \frac{1}{2N} (\mathbf{y} - X\mathbf{w})^T(\mathbf{y} - X\mathbf{w}) + \frac{\lambda}{2} ||\mathbf{w}||^2_2 \rightarrow$
$\mathbf{w^*_{ridge}} = (X^TX + 2N\lambda I)^{-1}X^T\mathbf{y}$\newline

Can be considered a MAP estimator : $\mathbf{w^*_{ridge}} = arg min_w - log(p(w|X,y))$

\subsection{Lasso}
Sparse solution.
$\mathcal{L}(w) = \frac{1}{2N} (y - Xw)^T(y - Xw) + \lambda ||w||_1 $

%========================================================================================================
% MODEL SELECTION
%========================================================================================================
\section{Model Selection}
\subsection{Bias-Variance decomposition}
Small dimensions : large bias, small variance.
Large dimensions : small bias, large variance.
Error for the val set compared to the emp distr of the data $\propto \sfrac{\sqrt{ln(|\Omega|)}}{\sqrt{|V|}}$


%========================================================================================================
% CLASSIFICATION
%========================================================================================================
\section{Classification}
\subsection{Optimal}
$\hat{y}(\mathbf{x}) = argmax_{y\in \mathcal{Y}} p(y|\mathbf{x})$

\subsection{Logistic regression}
$\sigma(z) = \frac{e^z}{1+e^z}$ to limit the predicted values $y\in [0;1]$ ($p(1|\mathbf{x}) = \sigma(\mathbf{x}^T\mathbf{w})$ and $p(0|\mathbf{x}) = 1-\sigma(\mathbf{x}^T\mathbf{w})$). Decision wrt $0.5$.

Likelihood

$p(y | X,w) = \prod p(y_n|x_n) \newline =\underset{n:y_n=0}{\prod} p(0|x_n) \dots \underset{n:y_n=K}{\prod} p(K|x_n) \newline = \prod_k \prod_n [p(y_n = k | x_n,w)]^{\tilde{y}_{nk}}$ \newline where ${\tilde{y}_{nk}} = 1$ if $y_n=k$.

For binary classification

$p(y | X, w) = \underset{n:y_n=0}{\prod} p(0|x_n) \dots\underset{n:y_n=1}{\prod} p(1|x_n) \newline = \underset{n}{\prod} \sigma({x_n^T w})^{y_n}[1-\sigma({x_n^T w})]^{1-y_n}$

Loss

$\mathcal{L}(w) = \sum_{n=1}^N ln(1 + exp(x_n^T w)) - y_n x_n^T w$ which is convex in $w$.

Gradient

$\nabla \mathcal{L}(w) = \sum_{n=1}^N x_n (\sigma(x_n^T w) - y_n) = X^T[\sigma(Xw) - y]$ (no closed form solution).

Hessian $H(w) = X^T S X$ \newline with $S_{nn} = \sigma(x_n^T w)[1-\sigma(x_n^T w)]$

\subsection{Exponential family}
General form

$p(y|\eta) = h(y) exp[\eta^T \psi(y) - A(\eta)]$

Cumulant

$A(\eta) = ln[\int_y h(y) exp[\eta^T \psi(y)] dy]$
\newline
$\nabla A(\eta) = \mathbb{E}[\psi(y)] = g^{-1}(\eta)$
\newline
$\nabla^2 A(\eta) = \mathbb{E}[\psi\psi^T] - \mathbb{E}[\psi]\mathbb{E}[\psi^T]$

Link function

$\eta = g(\mu) \Leftrightarrow \mu = g^{-1}(\eta)$

$\eta_{gaussian} = (\sfrac{\mu}{\sigma^2}, -\sfrac{1}{2 \sigma^2})$
; $\eta_{poisson} = ln(\mu)$
; $\eta_{bernoulli} = ln(\sfrac{\mu}{1 - \mu})$
\newline $\eta_{general} = g^{-1}(\frac{1}{N} \sum_{n=1}^N \psi(y_n))$

$\nabla \mathcal{L}(w) X^T [g^{-1}(Xw)- \psi(y)]= 0$

\subsection{Nearest Neighbour Models}
Performs best in low dimensions.
\subsubsection{k-NN}
$f_{S^{t,k}}(x) = \frac{1}{k} \sum_{n:x_n\in ngbh_{S{t,k}(x)}} y_n$
Pick odd $k$ so there is a clear winner.
Large $k \rightarrow$ large bias small variance (inv.)

\subsubsection{Error bound}
$\mathbb{E}[\mathcal{L}_{St}] \le 2 \mathcal{L}_{f^*} + 4 c \sqrt{d} N^{-1/d+1}$

\subsection{Support Vector Machines (SVM)}
Logistic regression with hinge loss :
$min_w \sum_{n=1}^N [1-y_n x_n^T w]_+ + \frac{\lambda}{2} ||w||^2$ where $y \in [-1;1]$ the label and $hinge(x)= max\{0,x\}$. Convex but not differentiable so need subgradient.

Duality : $\mathcal{L}(w) = max_{\alpha} G(w, \alpha)$. For SVM
$min_{w} max_{\alpha \in [0,1]^N} \sum \alpha_n (1 - y_nx_n^Tw) + \sfrac{\lambda}{2} ||w||^2$ differentiable and convex. Can switch $max$ and $min$ when convex in $w$ and concave in $\alpha$. Simpler form:\newline
$w(\alpha) = \frac{1}{\lambda} \sum \alpha_n y_n x_n = \frac{1}{\lambda} X^T diag(y) \alpha$ which yields the optimisation problem:\newline
$max_{\alpha \in [0,1]^N} \alpha^T\mathbf{1} - \sfrac{1}{2\lambda} \alpha^T Y X X^T Y \alpha$\newline


The solution is sparse ($\alpha_n$ is the slope of the lines that are lower bounds to the hinge loss).

\subsection{Kernel Ridge Regression}
From duality $w^* := X^T \alpha^*$ where $\alpha^* := (K + \lambda I_N)^{-1}y$ and $K=XX^T = \phi^T(x) \phi(x) = \kappa(x,x')$ (needs to be PSD and symmetric).


%========================================================================================================
% UNSUPERVISED LEARNING
%========================================================================================================
\section{Unsupervised Learning}
\subsection{K-means clustering}

$min\,\,\mathcal{L}(z,\mu) = \sum_n^N \sum_k^K z_{nk} ||x_n - \mu_k||^2_2$ with $z_{nk} \in \{0,1\}$ (unique assignments: $\sum_k z_{nk} = 1$).

Algorithm (Coordinate Descent)

1. $\forall n,$ $z_n = \begin{cases}
	1 \text{ if } k = argmin_j || x_n - \mu_k||^2 \\
	0 \text{ otherwise }
   \end{cases}$
2. $\forall k$ compute $\mu_k = \sfrac{\sum_n z_{nk} x_n}{\sum_n z_{nk}}$

Pb:cost,spher+hard clusters

Probabilistic model

$p(X|\mu,z) = \underset{n}{\prod}  \mathcal{N}(x_n|\mu_k,I) \newline= \underset{n}{\prod} \underset{k}{\prod}  \mathcal{N}(x_n|\mu_k,I)^{z_{nk}}$

\subsection{Gaussian Mixture Models}
$p(X|\mu,z) = \underset{n}{\prod} (x_n|z_n,\mu_k,\Sigma_k)p(z_n|\pi)
= \underset{n}{\prod}\underset{k}{\prod} [\mathcal{N}(x_n|\mu_k,\Sigma_k)]^{z_{nk}} \underset{k}{\prod} [\pi_k]^{z_{nk}}$ \newline where $pi_k = p(z_n=k)$

Marginal likelihood: $z_n$ latent variables => factored out of likelihood \newline $p(x_n|\theta) = \sum \pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)$.\newline nb params $O(N)$ to $O(D^2K)$.

\subsection{EM}
\subsubsection{GMM}
Intialize $\mu^{(1)}, \Sigma^{(1)}, \pi^{(1)}$.

1. E-step: Compute the assignments. $q_{kn}^{(t)} := \frac{\pi_k^{(t)} \mathcal{N}(x_n|\mu_k^{(t)}, \Sigma_k^{(t)})}{ \sum_k^K\pi_k^{(t)} \mathcal{N}(x_n|\mu_k^{(t)}, \Sigma_k^{(t)}) }$
2. Compute Marginal Likelihood
3. M-step: Update

$\mu^{(t+1)} = \frac{\underset{n}{\sum} q_{kn}^{(t)} x_n}{\underset{n}{\sum} q_{kn}^{(t)}}$ $\pi^{(t+1)} = \frac{1}{N} \underset{n}{\sum} q_{kn}^{(t)}$

$\Sigma^{(t+1)} = \frac{\underset{n}{\sum} q_{kn}^{(t)}(x_n - \mu^{(t+1)})(x_n - \mu^{(t+1)})^T}{\underset{n}{\sum} q_{kn}^{(t)}}$
\newline

\subsubsection{General}
$\theta^{(t+1)} := argmax_{\theta} \underset{n}{\sum} \mathbb{E}_{p(z_n|x_n,\theta^{(t)})}\newline[log\,\, p(x_n,z_n|\theta)]$\newline

%========================================================================================================
% Matrix Factorizations
%========================================================================================================
\section{Matrix Factorisations}
\subsection{Prediction}
Find $\mathbf{X} \approx \mathbf{W}\mathbf{Z}^T$ where $\mathbf{W} \in \mathbb{R}^{D\times K}$ and $\mathbf{Z} \in \mathbb{R}^{N\times K}$ with $K << D,N$. Large $K\rightarrow$ overfitting. If $K \ge max\{D,N\}$ trivial solution ($W=\mathbf{1}_D$ or $Z=\mathbf{1}_N$).

Quality of reconstruction (not jointly convex nor identifiable):\newline $\mathcal{L}(\mathbf{W}, \mathbf{Z}) := \frac{1}{2} \underset{(d,n)\in \Omega}{\sum} [x_{dn} - (\mathbf{W}\mathbf{Z}^T)_{dn}]^2 \newline= \underset{(d,n)\in \Omega}{\sum} f_{dn}(w,z)$

Regulariser: $\Omega(W,Z) = \frac{\lambda_w}{2} ||\mathbf{W}||^2_{Frob} + \frac{\lambda_z}{2} ||\mathbf{Z}||^2_{Frob}$

Optimisation with SGD (compute $\nabla_w$ for a fixed user $d'$ and $\nabla_z$ for a fixed item $n'$). 

ALS (assume no missing ratings):

$\mathbf{Z}^T_* = (\mathbf{W}^T\mathbf{W} + \lambda_z I_K)^{-1} \mathbf{W}^T \mathbf{X}$

$\mathbf{W}^T_* = (\mathbf{Z}^T\mathbf{Z} + \lambda_w I_K)^{-1} \mathbf{Z} \mathbf{X}^T$

% \subsection{Text Representation}
% Factorize the co-occurence matrix to get each row forming a representation of a word ($\mathbf{W}$) or a context word ($\mathbf{Z}$) respectively.
% \subsubsection{GloVe}
% $f_{dn} := min\{1,(n_{dn}/n_{max})^\alpha\}, \alpha \in [0;1]$
% \subsubsection{Skipgram/CBOW}
% Binary classification to separate real word pairs from fake ones.
% \subsection{FastText}
% Supervised sentence-level BoW.

%========================================================================================================
% DIM RED
%========================================================================================================
\section{Dimensionality reduction}
\subsection{SVD}
$\mathbf{X} = \mathbf{U} \mathbf{S} \mathbf{V}^T$, with $\mathbf{X}: D\times N$, $\mathbf{U}: D\times D$ orthonormal, $\mathbf{V}: N\times N$ orthonormal, $\mathbf{S}: D\times N$ diagonal PSD, values in descending order ($s_1 \ge \dots \ge s_D \ge 0$).

Reconstruction\newline $||\mathbf{X}-\mathbf{\hat{X}}||_F^2 \ge ||\mathbf{X}-\mathbf{U}_K\mathbf{U}_K^T\mathbf{X}||_F^2 = \underset{i\ge K+1}{\sum} s_i^2$ 
\newline$\forall$ rank-$K$ matrix $\mathbf{\hat{X}}$ (i.e. \textit{we should compress the data by projecting it onto these left singular vectors.})

Truncated SVD: $\mathbf{U}_K \mathbf{U}_K^T \mathbf{X} = \mathbf{U} \mathbf{S}_K \mathbf{V}^T$

Application to MF: $\mathbf{U} = \mathbf{W}$, $\mathbf{S}\mathbf{V}^T = \mathbf{Z}^T$. Rec. limited by the rank-K of W,Z.

\subsection{PCA}
Decorrelate the data. Empirical cov before: $N\mathbf{K} = \mathbf{X}\mathbf{X}^T = \mathbf{U}\mathbf{S}_D^2\mathbf{U}^T$.
After $\mathbf{\tilde{X}} = \mathbf{U}^T\mathbf{X}$: $N\mathbf{\tilde{K}} = \mathbf{\tilde{X}}\mathbf{\tilde{X}}^T = \mathbf{S}_D^2$ (the components are uncorrelated).

Pitfalls: not invariant under scalings.

%========================================================================================================
% Neural nets
%========================================================================================================
\section{Neural Networks}
The output at the node $j$ in layer $l$ is $x_j^{(l)} = \phi \big(\sum_i w_{i,j}^{(l)}x_i^{(l-1)} + b_j^{(l)} \big)$
\subsection{Representation power}
Error bound $\le \frac{(2Cr)^2}{n}$ where $C$ is the smoothness bound, $n$ the number of nodes. We can approximate any sufficiently smooth 2D function on bounded domain (\textit{on average} with $\sigma$ activation, \textit{pointwise} with ReLU).
\subsection{Learning}
Problem is not convex but SGD is stable. Backpropagation:
Let $\mathcal{L}_n = (y_n - f^{(L+1)} \circ \dots \circ f^{(1)}(\mathbf{x}^{(0)}_n))^2$.\newline
\textbf{Forward pass} \newline $\mathbf{x}^{(0)}=\mathbf{x}_n$. For $l=1,\dots,L+1$ \newline
$\mathbf{z}^{(l)} = (\mathbf{W}^{(l)})^T\mathbf{x}^{(l-1)}+\mathbf{b}^{(l)}$, $\mathbf{x}^{(l)} = \phi(\mathbf{z}^{(l)})$\newline
\textbf{Backward pass} \newline $\delta^{(L+1)} = -2(y_n-\mathbf{x}^{(L+1)})\phi^{'}(\mathbf{z}^{(L+1)})$ and $\forall l$ :
$\delta^{(l)} = (\mathbf{W}^{(l+1)}\delta^{(l+1)}) \odot \phi^{'}(\mathbf{z}^{(l)})$ \newline

\textbf{Final pass} \newline
$\frac{\partial{L}_n}{\partial w_{i,j}^{(l)}} = \delta^{(l)}_j \mathbf{x}_i^{(l-1)}$, $\frac{\partial{L}_n}{\partial b_{j}^{(l)}} = \delta_j^{(l)}$

\subsection{Activations}
sigmoid $\phi(x) = 1 -\sigma(x)$, tanh $\frac{e^x+e^{-x}}{e^x+e^{-x}} = 2 \phi(2x)-1$, ReLU, Leaky ReLU ($max\{\alpha x, x\}$).

\subsection{Convolutional Neural Nets}
Convolution with filter $f$: $x^{(1)}[n,m] = \sum_{k,l} f[k,l]x^{(0)}[n-k, m-l]$. Filter is local so no need for fully connected layers. We can use same filter at every position: \textit{weight sharing}. Learning: run backprop by computing different weights, then sum the gradients of shared weights.

\subsection{Overfitting}
Adding regularisation is equivalent to weight decay (by $(1-\eta\lambda)$). Can also use dataset augmentation, dropout.


%========================================================================================================
% Graphical models
%========================================================================================================
\section{Graphical Models}
\subsection{Bayes Nets}
$p(X_1,\dots,X_D) = p(X_1)p(X_2|X_1)\dots\newline p(X_D|X_1,\dots,X_{D-1})$. One node is a random variable, directed edge from $X_j$ to $X_i$ if $X_j$ appears in the conditioning $p(X_i|\dots,X_j,\dots)$. The graph must be \textit{acyclic}.

Conditional independence: $p(X,Y)=p(X)p(Y)$ or given $Z$ $p(X,Y|Z)=p(X|Z)p(Y|Z)$.

\includegraphics[width=\linewidth]{figs/bayes.png}
1. $p(x_1, x_2, x_3) = p(x_3)p(x_1|x_3)p(x_2|x_3)$ : $x_1$ and $x_2$  indep. given $x_3$\newline
2. $p = p(x_1)p(x_3|x_1)p(x_2|x_3)$ : id.\newline
3. $p = p(x_1)p(x_2)p(x_3|x_1, x_2)$ : $x_1$ and $x_2$  \textbf{not} indep. given $x_3$

$X \rightarrow Y$ path blocked by $Z$ if it contains a variable such that either
1. variable is in $Z$ and it is head-to-tail or tail-to-tail
2. node is head-to-head and neither this node nor any of its descendants are in $Z$.

$X$ and $Y$ are D-sep. by $Z$ iff every path $X \rightarrow Y$ is blocked by $Z$.

$X$ conditionally indep. of $Y$ conditioned on the $Z$ if $X$ and $Y$ are D-sep. by $Z$. Indep. is symmetric.

% The Markov blanket of a node $X_i$ is the set of parents, children, and co-parents of the node $X_i$ (other parents of its children).
